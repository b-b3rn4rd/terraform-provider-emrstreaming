---
# generated by https://github.com/hashicorp/terraform-plugin-docs
page_title: "emrstreaming_step Resource - emrstreaming"
subcategory: ""
description: |-
  The resource emrstreaming_step adds a step to an existing EMR cluster. This resource is designed to submit streaming jobs that supposed to continuously run on a permanent EMR cluster.The emrstreaming_step acts as a continuous deployment resource to manage jobs lifecycle on an EMR cluster.A step in any status except RUNNING will be marked for replacement
---

# emrstreaming_step (Resource)

The resource *emrstreaming_step* adds a step to an existing EMR cluster. This resource is designed to submit streaming jobs that supposed to continuously run on a permanent EMR cluster.The *emrstreaming_step* acts as a continuous deployment resource to manage jobs lifecycle on an EMR cluster.A step in any status except RUNNING will be marked for replacement

## Example Usage

```terraform
// add a job (example-app) to the running cluster
resource "emrstreaming_step" "step_example" {
  cluster_id = "j-xxxxxxxxxx"
  step = {
    hadoop_jar_step = {
      args = [
        "spark-submit",
        "--name", "example-app",
        "--deploy-mode", "cluster",
        "--master", "yarn",
        "--driver-cores", 2,
        "--executor-memory", "6g",
        "--driver-memory", "12g",
        "--executor-cores", "1",
        "--num-executors", "1",
        "--conf", "spark.dynamicAllocation.enabled=true",
        "--conf", "spark.executorEnv.PYTHONPATH=app-site-packages",
        "s3://example-bucket-name/example-app/main.py"
      ]
      jar = "command-runner.jar"
    }
    health_check_monitor_period = 120
    name                        = "example-app"
    force_redeploy              = true
    pre_shutdown_wait_period    = 10
    shutdown_timeout            = 60
  }
}
```

<!-- schema generated by tfplugindocs -->
## Schema

### Required

- `cluster_id` (String) A unique string that identifies a cluster.
- `step` (Attributes) A step to be deployed to the cluster. (see [below for nested schema](#nestedatt--step))

### Read-Only

- `id` (String) Step id a unique string that identifies the step on a cluster.

<a id="nestedatt--step"></a>
### Nested Schema for `step`

Required:

- `force_redeploy` (Boolean) Force to always re-deploy step regardless of the current status
- `hadoop_jar_step` (Attributes) (see [below for nested schema](#nestedatt--step--hadoop_jar_step))
- `health_check_monitor_period` (Number) A period of time in seconds to monitor the submitted job to become RUNNING
- `name` (String) The name of the step
- `shutdown_timeout` (Number) A timeout value in seconds for a job to become cancelled

Optional:

- `pre_shutdown_wait_period` (Number) A period of time in seconds to wait before cancelling the job. Use this parameter when utilizing your own shutdown approach

Read-Only:

- `status` (String) The current status of the step, should be RUNNING for successfully submitted jobs

<a id="nestedatt--step--hadoop_jar_step"></a>
### Nested Schema for `step.hadoop_jar_step`

Required:

- `jar` (String) A path to a JAR file run during the step.

Optional:

- `args` (List of String) A list of command line arguments to pass to the step.
- `main_class` (String) The name of the main class in the specified Java file.
- `properties` (Map of String) A list of Java properties that are set when the step runs. You can use these properties to pass key value pairs to your main function.


